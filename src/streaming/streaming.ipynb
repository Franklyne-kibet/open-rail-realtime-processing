{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.Spark Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1,com.datastax.spark:spark-cassandra-connector_2.12:3.3.0\" --conf \"spark.sql.extensions=com.datastax.spark.connector.CassandraSparkExtensions\" pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import regexp_replace, col, from_json\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark-Notebook\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"172.20.0.4\") \\\n",
    "    .config(\"spark.cassandra.connection.port\", \"9042\") \\\n",
    "    .config(\"spark.cassandra.output.consistency.level\", \"ONE\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set the configuration option\n",
    "spark.conf.set(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Reading From Kafka Stream\n",
    "\n",
    "through `readStream`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Raw Kafka Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default for startingOffsets is \"latest\"\n",
    "df_kafka_raw = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092,broker:29092\") \\\n",
    "    .option(\"subscribe\", \"train_data\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpoint\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kafka_raw.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Encoded Kafka Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kafka_encoded = df_kafka_raw.selectExpr(\"CAST(key AS STRING)\",\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kafka_encoded.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Structure Streaming DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the JSON string to PySpark schema\n",
    "train_schema = T.StructType(\n",
    "    [T.StructField(\"sequence_number\", T.StringType()),\n",
    "    T.StructField(\"schedule_id\", T.StringType()),\n",
    "    T.StructField(\"unique_id\", T.StringType()),\n",
    "    T.StructField(\"service_start_date\", T.StringType()),\n",
    "    T.StructField(\"location_code\", T.StringType()),\n",
    "    T.StructField(\"scheduled_arrival\", T.StringType()),\n",
    "    T.StructField(\"scheduled_departure\", T.StringType()),\n",
    "    T.StructField(\"actual_arrival\", T.StringType()),\n",
    "    T.StructField(\"actual_departure\", T.StringType()),\n",
    "    T.StructField(\"platforms\", T.StringType()),\n",
    "    T.StructField(\"estimated_time\", T.StringType()),\n",
    "    T.StructField(\"source\", T.StringType())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_train_from_kafka_message(df_raw, schema):\n",
    "    assert df_raw.isStreaming is True, \"DataFrame doesn't receive streaming data\"\n",
    "    # Convert the value column from JSON string to PySpark schema\n",
    "    df_json = df_raw.select(from_json(col(\"value\"), schema).alias(\"train\"))\n",
    "\n",
    "    # Flatten the nested columns and select the required columns\n",
    "    df_flattened = df_json.selectExpr(\n",
    "        \"CAST(train.sequence_number AS LONG) AS sequence_number\",\n",
    "        \"CAST(train.schedule_id AS LONG) AS schedule_id\",\n",
    "        \"train.unique_id\",\n",
    "        \"TO_DATE(train.service_start_date) AS service_start_date\",\n",
    "        \"train.location_code\",\n",
    "        \"train.scheduled_arrival\",\n",
    "        \"train.scheduled_departure\",\n",
    "        \"train.actual_arrival\",\n",
    "        \"train.actual_departure\",\n",
    "        \"train.platforms\",\n",
    "        \"train.estimated_time\",\n",
    "        \"train.source\"\n",
    "    )\n",
    "\n",
    "    return df_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trains = parse_train_from_kafka_message(df_raw=df_kafka_encoded, schema=train_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trains.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Write to Cassandra & Sink Operation\n",
    "\n",
    "through `writeStream`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Write to Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_cassandra(df):\n",
    "    if df.isStreaming:\n",
    "        write_query = df.writeStream \\\n",
    "            .foreachBatch(lambda batchDF, epochId: batchDF.write \\\n",
    "            .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .options(table=\"service_performance\", keyspace=\"train_service\") \\\n",
    "            .save()) \\\n",
    "            .start()\n",
    "        return write_query\n",
    "    else:\n",
    "        print(\"Data is not streaming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query = write_cassandra(df_trains)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Sink to Console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sink_console(df, output_mode: str = 'complete', processing_time: str = '5 seconds'):\n",
    "    write_query = df.writeStream \\\n",
    "        .outputMode(output_mode) \\\n",
    "        .trigger(processingTime=processing_time) \\\n",
    "        .format(\"console\") \\\n",
    "        .option(\"truncate\", False) \\\n",
    "        .start()\n",
    "    return write_query # pyspark.sql.streaming.StreamingQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query = sink_console(df_trains, output_mode='append')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
